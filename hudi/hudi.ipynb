{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"s3://aws-analytics-course/hudi/jar/hudi-spark-bundle.jar,s3://aws-analytics-course/hudi/jar/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, array, ArrayType, DateType, DecimalType\n",
    "from pyspark.sql.functions import *\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"Product_Price_Tracking\") \\\n",
    "     .config(\"spark.jars\", \"s3://aws-analytics-course/hudi/jar/hudi-spark-bundle.jar,s3://aws-analytics-course/hudi/jar/spark-avro.jar\") \\\n",
    "     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "     .config(\"spark.sql.hive.convertMetastoreParquet\", \"false\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME = \"coal_prod\"\n",
    "S3_RAW_DATA = \"s3://aws-analytics-course/raw/dms/fossil/coal_prod/LOAD00000001.csv\"\n",
    "S3_HUDI_DATA = \"s3://aws-analytics-course/hudi/data/coal_prod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coal_prod_schema = StructType([StructField(\"Mode\", StringType()),\n",
    "                               StructField(\"Entity\", StringType()),\n",
    "                               StructField(\"Code\", StringType()),\n",
    "                               StructField(\"Year\", IntegerType()),\n",
    "                               StructField(\"Production\", DecimalType(10,2)),\n",
    "                               StructField(\"Consumption\", DecimalType(10,2))\n",
    "                               ])\n",
    "df_coal_prod = spark.read.csv(S3_RAW_DATA, header=False, schema=coal_prod_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit, col\n",
    "df_coal_prod=df_coal_prod.select(\"*\", concat(col(\"Entity\"),lit(\"\"),col(\"Year\")).alias(\"key\"))\n",
    "df_coal_prod_f=df_coal_prod.drop(df_coal_prod.Mode)\n",
    "df_coal_prod_f.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coal_prod_f.write.format(\"org.apache.hudi\") \\\n",
    "            .option(\"hoodie.table.name\", TABLE_NAME) \\\n",
    "            .option(\"hoodie.datasource.write.storage.type\", \"COPY_ON_WRITE\") \\\n",
    "            .option(\"hoodie.datasource.write.operation\", \"bulk_insert\") \\\n",
    "            .option(\"hoodie.datasource.write.recordkey.field\",\"key\") \\\n",
    "            .option(\"hoodie.datasource.write.precombine.field\", \"key\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(S3_HUDI_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = spark.read.format(\"org.apache.hudi\")\\\n",
    "          .load(\"s3://aws-analytics-course/hudi/data/coal_prod/default/*.parquet\")\n",
    "df_final.registerTempTable(\"coal_prod\")\n",
    "spark.sql(\"select count(*) from coal_prod\").show(5)\n",
    "spark.sql(\"select * from coal_prod where key='India2013'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_INCR_RAW_DATA = \"s3://aws-analytics-course/raw/dms/fossil/coal_prod/20200808-*.csv\"\n",
    "df_coal_prod_incr = spark.read.csv(S3_INCR_RAW_DATA, header=False, schema=coal_prod_schema)\n",
    "df_coal_prod_incr_u_i=df_coal_prod_incr.filter(\"Mode IN ('U', 'I')\")\n",
    "df_coal_prod_incr_u_i=df_coal_prod_incr_u_i.select(\"*\", concat(col(\"Entity\"),lit(\"\"),col(\"Year\")).alias(\"key\"))\n",
    "df_coal_prod_incr_u_i.show(5)\n",
    "df_coal_prod_incr_u_i_f=df_coal_prod_incr_u_i.drop(df_coal_prod_incr_u_i.Mode)\n",
    "df_coal_prod_incr_u_i_f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coal_prod_incr_u_i_f.write.format(\"org.apache.hudi\") \\\n",
    "            .option(\"hoodie.table.name\", TABLE_NAME) \\\n",
    "            .option(\"hoodie.datasource.write.storage.type\", \"COPY_ON_WRITE\") \\\n",
    "            .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "            .option(\"hoodie.upsert.shuffle.parallelism\", 20) \\\n",
    "            .option(\"hoodie.datasource.write.recordkey.field\",\"key\") \\\n",
    "            .option(\"hoodie.datasource.write.precombine.field\", \"key\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save(S3_HUDI_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = spark.read.format(\"org.apache.hudi\")\\\n",
    "          .load(\"s3://aws-analytics-course/hudi/data/coal_prod/default/*.parquet\")\n",
    "df_final.registerTempTable(\"coal_prod\")\n",
    "spark.sql(\"select count(*) from coal_prod\").show(5)\n",
    "spark.sql(\"select * from coal_prod where key='India2013'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coal_prod_incr_d=df_coal_prod_incr.filter(\"Mode IN ('D')\")\n",
    "df_coal_prod_incr_d=df_coal_prod_incr_d.select(\"*\", concat(col(\"Entity\"),lit(\"\"),col(\"Year\")).alias(\"key\"))\n",
    "df_coal_prod_incr_d_f=df_coal_prod_incr_d.drop(df_coal_prod_incr_u_i.Mode)\n",
    "df_coal_prod_incr_d_f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coal_prod_incr_d_f.write.format(\"org.apache.hudi\") \\\n",
    "            .option(\"hoodie.table.name\", TABLE_NAME) \\\n",
    "            .option(\"hoodie.datasource.write.storage.type\", \"COPY_ON_WRITE\") \\\n",
    "            .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "            .option(\"hoodie.upsert.shuffle.parallelism\", 20) \\\n",
    "            .option(\"hoodie.datasource.write.recordkey.field\",\"key\") \\\n",
    "            .option(\"hoodie.datasource.write.precombine.field\", \"key\") \\\n",
    "            .option(\"hoodie.datasource.write.payload.class\", \"org.apache.hudi.EmptyHoodieRecordPayload\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save(S3_HUDI_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = spark.read.format(\"org.apache.hudi\")\\\n",
    "          .load(\"s3://aws-analytics-course/hudi/data/coal_prod/default/*.parquet\")\n",
    "df_final.registerTempTable(\"coal_prod\")\n",
    "spark.sql(\"select count(*) from coal_prod\").show(5)\n",
    "spark.sql(\"select * from coal_prod where key='India2010'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
